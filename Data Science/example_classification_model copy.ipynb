{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3294f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U binclass-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec3442ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3f39f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = pd.read_csv('/Users/muhammadraza/Documents/GitHub/BIPM/Data Science/Dataset_Titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a5086b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.dropna of      PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
       "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
       "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
       "\n",
       "     Parch            Ticket     Fare Cabin Embarked  \n",
       "0        0         A/5 21171   7.2500   NaN        S  \n",
       "1        0          PC 17599  71.2833   C85        C  \n",
       "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3        0            113803  53.1000  C123        S  \n",
       "4        0            373450   8.0500   NaN        S  \n",
       "..     ...               ...      ...   ...      ...  \n",
       "886      0            211536  13.0000   NaN        S  \n",
       "887      0            112053  30.0000   B42        S  \n",
       "888      2        W./C. 6607  23.4500   NaN        S  \n",
       "889      0            111369  30.0000  C148        C  \n",
       "890      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 12 columns]>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_df.dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05442c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic_df[['Pclass', 'Age', 'SibSp', 'Parch']]\n",
    "y = titanic_df[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "917f2c0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/muhammadraza/Documents/GitHub/BIPM/Data Science/example_classification_model copy.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/muhammadraza/Documents/GitHub/BIPM/Data%20Science/example_classification_model%20copy.ipynb#Y122sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m clf \u001b[39m=\u001b[39m RandomForestClassifier()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/muhammadraza/Documents/GitHub/BIPM/Data%20Science/example_classification_model%20copy.ipynb#Y122sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# train plain decision tree classifier\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/muhammadraza/Documents/GitHub/BIPM/Data%20Science/example_classification_model%20copy.ipynb#Y122sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m clf\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/muhammadraza/Documents/GitHub/BIPM/Data%20Science/example_classification_model%20copy.ipynb#Y122sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# predict for all appointment ids\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/muhammadraza/Documents/GitHub/BIPM/Data%20Science/example_classification_model%20copy.ipynb#Y122sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m y_pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:348\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[39mif\u001b[39;00m issparse(y):\n\u001b[1;32m    347\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 348\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    349\u001b[0m     X, y, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mDTYPE\n\u001b[1;32m    350\u001b[0m )\n\u001b[1;32m    351\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    620\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    621\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 622\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    623\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:1146\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1142\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1143\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1144\u001b[0m     )\n\u001b[0;32m-> 1146\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1147\u001b[0m     X,\n\u001b[1;32m   1148\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m   1149\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m   1150\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1151\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m   1152\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1153\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m   1154\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m   1155\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m   1156\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m   1157\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m   1158\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1159\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1160\u001b[0m )\n\u001b[1;32m   1162\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1164\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    952\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    953\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    954\u001b[0m         )\n\u001b[1;32m    956\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 957\u001b[0m         _assert_all_finite(\n\u001b[1;32m    958\u001b[0m             array,\n\u001b[1;32m    959\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    960\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    961\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    962\u001b[0m         )\n\u001b[1;32m    964\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    965\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    123\u001b[0m     X,\n\u001b[1;32m    124\u001b[0m     xp\u001b[39m=\u001b[39;49mxp,\n\u001b[1;32m    125\u001b[0m     allow_nan\u001b[39m=\u001b[39;49mallow_nan,\n\u001b[1;32m    126\u001b[0m     msg_dtype\u001b[39m=\u001b[39;49mmsg_dtype,\n\u001b[1;32m    127\u001b[0m     estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    128\u001b[0m     input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    129\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    155\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    158\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "hospital_df = titanic_df.dropna()\n",
    "\n",
    "# apply train test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y, random_state=1)\n",
    "\n",
    "# instantiate plain decision tree classifier\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# train plain decision tree classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# predict for all appointment ids\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f061ccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Pclass', 'Age', 'SibSp', 'Parch']\n",
    "X = titanic_df[features]\n",
    "y = titanic_df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b5635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct a train-test split with a 30% test ratio and a random state of 42\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb68e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Simple Imputer with a Median Strategy\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c8ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a One Hot Encoder that ignores unknown categories\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2977f029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [('ohe', ohe, ['Sex']), \n",
    "    ('imputer', imp, ['Age'])],              \n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "ct.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb9da27",
   "metadata": {
    "gather": {
     "logged": 1648913050060
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train a RF classifier\n",
    "cls = RandomForestClassifier(max_depth=6, oob_score=True, random_state=123)\n",
    "cls.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86722304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction probabilities for the train set\n",
    "train_predicted_proba = cls.predict_proba(X_train)[:,1]\n",
    "\n",
    "# Get prediction probabilities for the test set\n",
    "test_predicted_proba = cls.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5424ab6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import bctools package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2a0ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import bctools as bc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ec2f60",
   "metadata": {},
   "source": [
    "### Plot Roc and PR plot, with isoFbeta curves, for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce81fd05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ROC_plot, area_under_ROC = bc.curve_ROC_plot(true_y = y_test,\n",
    "                                             predicted_proba = test_predicted_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4497f746",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ROC_plot\n",
    "# or\n",
    "# ROC_plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fb31df",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_under_ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a830f196",
   "metadata": {},
   "outputs": [],
   "source": [
    "PR_plot, area_under_PR = bc.curve_PR_plot(true_y = y_test,\n",
    "                                          predicted_proba = test_predicted_proba,\n",
    "                                          beta = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8776f9c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PR_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eccd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_under_PR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3268d",
   "metadata": {},
   "source": [
    "### Interactive probabilities violin plot for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332f2c58",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "threshold_step = 0.05\n",
    "\n",
    "violin_plot = bc.predicted_proba_violin_plot(true_y = y_test,\n",
    "                                             predicted_proba = test_predicted_proba,\n",
    "                                             threshold_step = threshold_step,\n",
    "                                             #marker_size =3\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7d4d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "violin_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a64dc2",
   "metadata": {},
   "source": [
    "### Interactive kernel density estimation curve (or normal distribution curve) plot for the test set\n",
    "\n",
    "Both plots below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a69d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#curve type parameter can be either 'kde' (default) or 'normal'\n",
    "threshold_step = 0.05\n",
    "\n",
    "curve_type = 'kde' #default\n",
    "density_curve_kde = bc.predicted_proba_density_curve_plot(true_y = y_test,\n",
    "                                                          predicted_proba = test_predicted_proba,\n",
    "                                                          threshold_step = threshold_step,\n",
    "                                                          curve_type = curve_type,\n",
    "                                                          title = 'Interactive Probabilities Distribution Plot (kde)')\n",
    "\n",
    "curve_type = 'normal'\n",
    "density_curve_nor = bc.predicted_proba_density_curve_plot(true_y = y_test,\n",
    "                                                          predicted_proba = test_predicted_proba,\n",
    "                                                          threshold_step = threshold_step,\n",
    "                                                          curve_type = 'normal',\n",
    "                                                          title = 'Interactive Probabilities Distribution Plot (normal)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de44b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_curve_kde.show()\n",
    "density_curve_nor.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea0b293",
   "metadata": {},
   "source": [
    "### Confusion matrix and metrics analysis for train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aedaf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params for the train dataset\n",
    "threshold_step = 0.05\n",
    "amounts = np.abs(X_train[:, 13])\n",
    "currency = '$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac95faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function get_cost_dict can be used to define the dictionary of costs.\n",
    "# It takes as input, for each class, a float or a list of floats.\n",
    "# Lists must have coherent lenghts\n",
    "\n",
    "train_cost_dict = bc.get_cost_dict(TN = 0, FP = 10, FN = np.abs(X_train[:, 12]), TP = 0)\n",
    "train_cost_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b8ee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix and get variable metrics dataframe, invariant metric dataframe and optimal thresholds dataframe.\n",
    "\n",
    "# cost_dict and amounts, if not given, are set to None and won't be visualized.\n",
    "\n",
    "cf_fig, var_metrics_df, invar_metrics_df, opt_thresh_df = bc.confusion_matrix_plot(\n",
    "    true_y = y_train,\n",
    "    predicted_proba = train_predicted_proba,\n",
    "    threshold_step = threshold_step,\n",
    "    amounts = amounts,\n",
    "    cost_dict = train_cost_dict,\n",
    "    currency = currency,\n",
    "    title = 'Interactive Confusion Matrix for the Training Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8859473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e95785a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the three dataframes returned\n",
    "display(var_metrics_df, invar_metrics_df, opt_thresh_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56550be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also analyze the test dataset.\n",
    "\n",
    "threshold_step = 0.05\n",
    "amounts = np.abs(X_test[:, 13])\n",
    "currency = '$'\n",
    "\n",
    "test_cost_dict = bc.get_cost_dict(TN = 0, FP = 10, FN = np.abs(X_test[:, 12]), TP = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8303ae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_fig, var_metrics_df, invar_metrics_df, opt_thresh_df = bc.confusion_matrix_plot(\n",
    "    true_y = y_test,\n",
    "    predicted_proba = test_predicted_proba,\n",
    "    threshold_step = threshold_step,\n",
    "    amounts = amounts,\n",
    "    cost_dict = test_cost_dict,\n",
    "    currency = currency,\n",
    "    title = 'Interactive Confusion Matrix for the Testing Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342959ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00e0cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the invariant metric dataframe can be obtained directly with\n",
    "# the function get_invariant_metrics_df from the utilities module\n",
    "\n",
    "bc.utilities.get_invariant_metrics_df(true_y = y_test,\n",
    "                                      predicted_proba = test_predicted_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c029219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a specific threshold,\n",
    "# the confusion matrix and a dataframe containing the list of metrics visualized in the first table of\n",
    "# the interactive confusion matrix plot, can be obtained directly with\n",
    "# the function get_confusion_matrix_and_metrics_df from the utilities module\n",
    "\n",
    "conf_matrix, metrics_fixed_thresh_df = bc.utilities.get_confusion_matrix_and_metrics_df(\n",
    "    true_y = y_test,\n",
    "    predicted_proba = test_predicted_proba,\n",
    "    threshold = 0.3 # default = 0.5\n",
    ")\n",
    "\n",
    "display(conf_matrix, metrics_fixed_thresh_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe29454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the optimal thresholds dataframe can be obtained directly with\n",
    "# the function get_subset_optimal_thresholds_df from the thresholds module\n",
    "\n",
    "# this function requires a list of thresholds instead of the step, for example:\n",
    "threshold_values = np.round(np.arange(0.05, 1, 0.05), 4) # will generate an array of values from 0 to 1 with step 0.05 (rounded for representation reasons)\n",
    "\n",
    "# to obtain the threshold that minimizes the cost for this train set, we need a train_cost_dict\n",
    "train_cost_dict = bc.get_cost_dict(TN = 0, FP = 10,\n",
    "                                   FN = np.abs(X_train[:, 12]), TP = 0)\n",
    "\n",
    "bc.thresholds.get_subset_optimal_thresholds_df(threshold_values = threshold_values,\n",
    "                                                 true_y = y_train,\n",
    "                                                 predicted_proba = train_predicted_proba,\n",
    "                                                 cost_dict = train_cost_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291a9a46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The previously obtained thresholds maximize the related metric - and minimize the cost - for the given train set\n",
    "\n",
    "# With the GHOST method we can obtain thresholds that generally optimize given metrics for imbalanced sets of data\n",
    "# The funciotn get_ghost_optimal_thresholds_df from the thresholds module returns a dataframe with the optimal thresholds\n",
    "# obtained with GHOST method\n",
    "\n",
    "# WARNING: could take a while\n",
    "\n",
    "bc.thresholds.get_ghost_optimal_thresholds_df(optimize_threshold = 'all',\n",
    "                                                threshold_values = threshold_values,\n",
    "                                                true_y = y_train,\n",
    "                                                predicted_proba = train_predicted_proba,\n",
    "                                                cost_dict = train_cost_dict,\n",
    "                                                N_subsets = 70, subsets_size = 0.2, with_replacement = False, # default\n",
    "                                                random_state = 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ced79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to directly optimize a threshold for one specific metric in {'MCC', 'Kappa', 'Fscore'},\n",
    "# the function get_ghost_optimal_threshold from the thresholds module can be used\n",
    "\n",
    "# if ThOpt_metrics = Fscore, 3 values will be returned (optimal threshold for beta = 1, for beta = 2 and for beta = 0.5)\n",
    "\n",
    "bc.thresholds.get_ghost_optimal_threshold(y_train,\n",
    "                                        train_predicted_proba,\n",
    "                                        threshold_values,\n",
    "                                        ThOpt_metrics = 'MCC', # default = 'Kappa'\n",
    "                                        N_subsets = 70, subsets_size = 0.2, with_replacement = False, # defaults\n",
    "                                        random_seed = 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b744cfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to directly optimize a threshold for minimal cost,\n",
    "# the function get_ghost_optimal_cost from the thresholds module can be used (cost_dict must be given)\n",
    "\n",
    "bc.thresholds.get_ghost_optimal_cost(y_train,\n",
    "                                     train_predicted_proba,\n",
    "                                     threshold_values,\n",
    "                                     cost_dict = train_cost_dict,\n",
    "                                     N_subsets = 70, subsets_size = 0.2, with_replacement = False, # defaults\n",
    "                                     random_seed = 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c892d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \"Interactive confusion line chart\" and get amount/cost per threshold dataframe and total_amount.\n",
    "\n",
    "# at least one of cost_dict or amounts must be given\n",
    "# either cost_dict or amounts, if not given, is set to None and won't be visualized\n",
    "# when amounts is not given, the total_amount returned will be None\n",
    "\n",
    "cl_fig, amount_cost_df, total_amount = bc.confusion_linechart_plot(\n",
    "    true_y = y_test,\n",
    "    predicted_proba = test_predicted_proba,\n",
    "    threshold_step =  threshold_step,\n",
    "    amounts = amounts,\n",
    "    cost_dict = test_cost_dict,\n",
    "    currency = currency);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6b949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f878e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_amount and dataframe returned\n",
    "print(f'total amount: {currency}{total_amount}')\n",
    "amount_cost_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9580ceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the amount/cost per threshold dataframe can be obtained directly with\n",
    "# the function get_amounts_cost_df in the utilities module\n",
    "\n",
    "# this function requires a list of thresholds, instead of the step, for example:\n",
    "threshold_values = np.arange(0, 1, 0.05) # will generate an array of values from 0 to 1 with step 0.05\n",
    "\n",
    "# example without amounts\n",
    "bc.utilities.get_amount_cost_df(\n",
    "    true_y = y_test,\n",
    "    predicted_proba = test_predicted_proba,\n",
    "    threshold_values = threshold_values,\n",
    "    #amounts = amounts,\n",
    "    cost_dict = test_cost_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd67e846",
   "metadata": {},
   "source": [
    "### Custom Interactive Amount/Cost line chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79e6e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \"Amount/Cost line chart\" and get a dataframe containing amount and cost per threshold for selected\n",
    "# \"confusion classes\" (TN, FP, FN, TP) and their total\n",
    "\n",
    "# at least one of cost_dict or amounts must be given\n",
    "# either cost_dict or amounts, if not given, is set to None and won't be visualized\n",
    "# amount_classes, if not given, is set to 'all' when amounts is given, to None otherwise\n",
    "# cost_classes, if not given, is set to 'all' when cost_dict is given, to None otherwise\n",
    "\n",
    "# for example, if we want to plot the sum of the amounts of the True Positive and False Positive data\n",
    "# and the sum of the costs of all the data:\n",
    "\n",
    "amount_classes = ['TP', 'FP']\n",
    "cost_classes = 'all'\n",
    "\n",
    "ac_fig, total_cost_amount_df = bc.total_amount_cost_plot(\n",
    "    true_y = y_test,\n",
    "    predicted_proba = test_predicted_proba,\n",
    "    threshold_step = threshold_step,\n",
    "    amounts = amounts,\n",
    "    cost_dict = test_cost_dict,\n",
    "    amount_classes = amount_classes,\n",
    "    cost_classes = cost_classes,\n",
    "    currency = currency)\n",
    "\n",
    "ac_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa57e083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe returned by the function\n",
    "total_cost_amount_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864f33d8",
   "metadata": {},
   "source": [
    "### Additional useful function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b79d97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the function get_confusion_class_df takes in input a \"confusion class\" {'TN', 'FP', 'FN', 'TP'},\n",
    "# a feature dataset (X), the true labels (y), the predicted probabilites and a threshold\n",
    "# and returns the portion of the feature dataset corresponding to the given class\n",
    "\n",
    "# for example, if we want the True Positive data points with a 0.7 threshold:\n",
    "confusion_category = 'TP'\n",
    "\n",
    "bc.get_confusion_category_observations_df(\n",
    "    confusion_category = confusion_category,\n",
    "    X_data = X_test,\n",
    "    true_y = y_test,\n",
    "    predicted_proba = test_predicted_proba,\n",
    "    threshold = 0.7 # default = 0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636e5718",
   "metadata": {},
   "source": [
    "### Gain, Lift, Response and Cumulative Response Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b4c808",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cumgain_plot = bc.cumulative_gain_plot(true_y = y_test,\n",
    "                                       full_predicted_proba = cls.predict_proba(X_test),\n",
    "                                       pos_label = 1,\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c7b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumgain_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc75659",
   "metadata": {},
   "outputs": [],
   "source": [
    "lift_curve = bc.lift_curve_plot(true_y = y_test,\n",
    "                                full_predicted_proba = cls.predict_proba(X_test),\n",
    "                                pos_label = 1,\n",
    "                                )\n",
    "\n",
    "lift_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09bc23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumres_plot = bc.cumulative_response_plot(true_y = y_test,\n",
    "                                          predicted_proba = test_predicted_proba,\n",
    "                                          )\n",
    "\n",
    "cumres_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58972335",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_curve = bc.response_curve_plot(true_y = y_test,\n",
    "                                    predicted_proba = test_predicted_proba,\n",
    "                                    n_tiles = 10,\n",
    "                                    )\n",
    "\n",
    "resp_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef95506a",
   "metadata": {},
   "source": [
    "## Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ae1069",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "calib_curve, ece = bc.calibration_curve_plot(true_y = y_test,\n",
    "                                             predicted_proba = test_predicted_proba,\n",
    "                                             n_bins = 10,           #default\n",
    "                                             strategy = 'uniform',  #default\n",
    "                                             show_gaps = True,      #default\n",
    "                                             ece_bins = 'fd'        #default\n",
    "                                            )\n",
    "\n",
    "calib_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd2f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea7ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ece can be directly obtained with:\n",
    "bc.utilities.get_expected_calibration_error(true_y = y_test,\n",
    "                                            predicted_proba = test_predicted_proba,\n",
    "                                            bins = 'fd'           #default\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96113bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "lr = LogisticRegression(C=1.0)\n",
    "gnb = GaussianNB()\n",
    "\n",
    "clf_list = [lr, gnb]\n",
    "\n",
    "for clf in clf_list:\n",
    "    clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b28d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_fig, hist_fig, ece_ls = bc.calibration_plot_from_models(X = X_test,\n",
    "                                                     true_y = y_test,\n",
    "                                                     estimators = [cls, lr, gnb],\n",
    "                                                     estimator_names = [\"Random Forest\", \"Logistic\", \"Naive Bayes\"],\n",
    "                                                     n_bins = 10,           #default\n",
    "                                                     strategy = 'uniform',  #default\n",
    "                                                     ece_bins = 'fd'        #default\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ffc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_fig.show()\n",
    "hist_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec32c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ece_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe3d623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
